{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8626203a",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000d9334",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9780/2906289442.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import calendar\n",
    "import dash\n",
    "from dash import dcc, html, dcc, callback_context\n",
    "import dash_core_components as dcc\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_html_components as html\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.tsa.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from textblob import TextBlob\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a79735",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719c762",
   "metadata": {},
   "source": [
    "# Importing Dataset\n",
    "#### This dataset will be used for complete Data Visualisation task and  for the Question 1 (Time Series) from Machine Learning for Business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d5e71",
   "metadata": {},
   "source": [
    "#### For better understanding of the dataset, I'm providing terminology explanation, to be aware what each weather parameter represents:\n",
    "\n",
    "datetime: Date and time of the weather data.\n",
    "\n",
    "tempmax: Maximum temperature recorded.\n",
    "\n",
    "tempmin: Minimum temperature recorded.\n",
    "\n",
    "temp: Current temperature.\n",
    "\n",
    "feelslikemax: Maximum \"feels like\" temperature.\n",
    "\n",
    "feelslikemin: Minimum \"feels like\" temperature.\n",
    "\n",
    "feelslike: Current \"feels like\" temperature.\n",
    "\n",
    "dew: Dew point temperature.\n",
    "\n",
    "humidity: Relative humidity percentage.\n",
    "\n",
    "precip: Precipitation amount.\n",
    "\n",
    "precipprob: Probability of precipitation.\n",
    "\n",
    "precipcover: Extent of precipitation coverage.\n",
    "\n",
    "preciptype: Type of precipitation (e.g., rain, snow).\n",
    "\n",
    "snow: Snowfall amount.\n",
    "\n",
    "snowdepth: Snow depth on the ground.\n",
    "\n",
    "windgust: Maximum wind gust.\n",
    "\n",
    "windspeed: Current wind speed.\n",
    "\n",
    "winddir: Wind direction.\n",
    "\n",
    "sealevelpressure: Atmospheric pressure at sea level.\n",
    "\n",
    "cloudcover: Percentage of sky covered by clouds.\n",
    "\n",
    "visibility: Visibility range in meters.\n",
    "\n",
    "solarradiation: Solar radiation amount.\n",
    "\n",
    "solarenergy: Solar energy captured or available.\n",
    "\n",
    "uvindex: UV index.\n",
    "\n",
    "severerisk: Risk of severe weather conditions.\n",
    "\n",
    "sunrise: Time of sunrise.\n",
    "\n",
    "sunset: Time of sunset.\n",
    "\n",
    "moonphase: Current phase of the moon.\n",
    "\n",
    "conditions: Summary of weather conditions.\n",
    "\n",
    "description: Detailed description of weather conditions.\n",
    "\n",
    "icon: Icon representing weather conditions.\n",
    "\n",
    "stations: Information about weather monitoring stations or locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9bcb22",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe84079",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "nan_count_per_column = df[columns_with_nan].isnull().sum()\n",
    "\n",
    "print(\"Columns with NaN values and their respective counts:\")\n",
    "print(nan_count_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "for column in columns_with_nan:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Column '{column}':\")\n",
    "    print(unique_values)\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03da98",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1ce33",
   "metadata": {},
   "source": [
    "I looked for the unique values within columns that contain NaN values, to see if missing value is just matter of mistake or it indicates that value for that column was not available in the moment of recording, since it's weather data. \n",
    "\n",
    "1. precitype (Precipitation Type) It has 1003 NaN values. Missing values here suggest that the type of precipitation (rain, snow etc.) is not specified or availabe at the moment of recording. It might happen due to not having rain or snow at that moment at all. However, this column doesn't bring much value for my dataset and will drop it. \n",
    "\n",
    "\n",
    "2. snowdepth (Snow Depth): There is only 1 NaN value in this column. Snow depth typically represents the depth of accumulated snow at a specific location or time. One missing value indicates that snow depth data was not added as we have 0 within unique values. I will replace NaN value by using Interpolation.\n",
    "\n",
    "\n",
    "3. windgust (Wind Gust): This column contains 254 missing values. Wind gust refers to the maximum wind speed observed over a brief period. The absence of values in this column for 254 observations signifies that wind gust data is not available for these instances, so I will replace thos NaN values by using Interpolation. \n",
    "\n",
    "\n",
    "4. sealevelpressure (Sea Level Pressure): There is 1 missing value in this column. Sea level pressure represents atmospheric pressure adjusted to sea level. One missing value implies that sea level pressure data was simply not added that day. I will replace NaN value using Interpolation. \n",
    "\n",
    "\n",
    "5. severerisk (Risk of Severe Weather Conditions): This column has 1374 missing values. Severerisk represents the risk level associated with severe weather conditions. The high count of missing values (1374) indicates that the risk level for severe weather was not applicable for these recordings, therefore NaN values as there was no risk level. In this situation, I will replace NaN values with 0 by using Imputation. \n",
    "\n",
    "\n",
    "6. stations: There are 6 missing values in the 'stations' column. This column might contain identifiers or codes for weather stations. The missing values suggest that the station identifiers are not added for these observations. Since this column doesn't bring any value to my dataset, as there are no longitude and altitude for plotting, I will just remove it from dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c4b67",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e55d1",
   "metadata": {},
   "source": [
    "### Removing NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95522baf",
   "metadata": {},
   "source": [
    "#### Droppping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['precip', 'precipprob', 'precipcover', 'preciptype', 'stations']\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a999fb2",
   "metadata": {},
   "source": [
    "#### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3268262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sealevelpressure'].interpolate(method='linear', inplace=True)\n",
    "df['windgust'].interpolate(method='linear', inplace=True)\n",
    "df['snowdepth'].interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e01f0",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['severerisk'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de24df",
   "metadata": {},
   "source": [
    "#### Re-checking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values = df.isnull().sum()\n",
    "nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f292e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['sunrise'] = pd.to_datetime(df['sunrise'])\n",
    "df['sunset'] = pd.to_datetime(df['sunset'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3980669",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ada5d8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed164d",
   "metadata": {},
   "source": [
    "#### Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_histogram = ['tempmax', 'tempmin', 'temp', 'humidity', 'windgust', 'uvindex']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(columns_for_histogram, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.hist(df[col], bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.title(col + \" Distribution\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_histogram = ['tempmax', 'tempmin', 'temp', 'humidity', 'windgust', 'uvindex']\n",
    "fig = px.histogram(df, x=columns_for_histogram, marginal=\"rug\", title=\"Distribution of Weather Parameters\")\n",
    "fig.update_layout(bargap=0.3)  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b1a0d",
   "metadata": {},
   "source": [
    "As we can see from the distribution plots, most common 'tempmax' is around 30, which indicates that frequently occuring maximum temperature recorded is around 30 degrees Celsius. Most common minimum temperature is around 22C and most common temperature is around 25.\n",
    "\n",
    "Most common humidity is in range between 70-75%, most common wind gusts are 15 to 20 km/h and most common UV index values are 8 and 9, which represent quite hing exposure. \n",
    "\n",
    "Based on these information, weather conditions in this area could be described as moderately warm, with moderate humidity and wind gusts. This would imply comfortable climate with temperatures typically not too hot or cold, along with moderate humidity levels and occasional breezy conditions. We can see on minimum temperature that it barely goes under 0C. Also, due to high UV index, it would be advisable to take precautions when UV index is in most common range (8-9).\n",
    "\n",
    "\n",
    "About the plot:\n",
    "\n",
    "I created a histogram to visualise the distributions of numerical variables in my dataset. For this plot, I used probably the most relevant ones: 'tempmax', 'tempmin', 'temp', 'humidity' and 'windgust'. \n",
    "\n",
    "I used 'blue' color for better visibility and aesthetic appeal in visualisations. I also added 'edgecolor=black' to define histogram bars edges for better distinction and readability. Also, as per feedback from CA1, I added Grid Lines for better readability of values and visual interpreting the distribution of data. In the plot under, I created a interactive version static plot, for dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779a2a3",
   "metadata": {},
   "source": [
    "#### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fae43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = ['temp', 'humidity', 'windgust', 'uvindex', 'severerisk']\n",
    "pairplot_data = df[continuous_columns]\n",
    "\n",
    "fig = px.scatter_matrix(pairplot_data, dimensions=continuous_columns, title='Correlation between Continuous Variables')\n",
    "\n",
    "fig.update_traces(diagonal_visible=False)  \n",
    "fig.update_layout(width=1000, height=800)  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09424e2d",
   "metadata": {},
   "source": [
    "Correlations between columns:\n",
    "\n",
    "1. Logic correclation comes between UV index and temperature - higher UV index is noted on higher temperatures, but also, most of the high UV index values are noted between 20 and 30C. Higher severe risk was noted only on higher temperatures. There is no specific correlation really between temperature and wind gust or humidity, except that wind gusts and humidity mostly show up when temperature is above 0C.\n",
    "\n",
    "2. Humidity doesn't have much impact on UV index, but high UV index appears mostly when humidity is less than 80%. High severity risks are appearing mostly on higher humidity levels. \n",
    "\n",
    "3. Wind gusts have no much correlation with other factors. High severe risks usually show up on wind gusts between 25-50 km/h."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38e11c",
   "metadata": {},
   "source": [
    "#### Categorical Variables Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16978b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "icon_counts = df['icon'].value_counts().reset_index()\n",
    "icon_counts.columns = ['Icon', 'Count']  # Rename the columns for Plotly compatibility\n",
    "\n",
    "fig = px.bar(\n",
    "    icon_counts,\n",
    "    x='Icon',\n",
    "    y='Count',\n",
    "    labels={'Icon': 'Icon', 'Count': 'Count'},\n",
    "    title='Distribution of Weather Icons'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis={'tickangle': 45},\n",
    "    yaxis={'title': 'Count'},\n",
    "    height=500,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea34eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "fig = px.scatter(df, x='datetime', y='icon', labels={'icon': 'Icon', 'datetime': 'Date'},\n",
    "                 title='Type of Weather for Each Day', hover_data={'datetime': '|%Y-%m-%d'})\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis={'title': 'Date'},\n",
    "    yaxis={'title': 'Icon'},\n",
    "    height=400,  # Adjusted height for dashboard fitting\n",
    "    width=700,   # Adjusted width for dashboard fitting\n",
    "    template='plotly_white',  # Lighter theme for better dashboard integration\n",
    "    margin=dict(l=40, r=40, t=40, b=40),  # Adjusted margins for better space utilization\n",
    "    font=dict(family=\"Arial\", size=12),  # Font adjustments for better readability\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429b1c0",
   "metadata": {},
   "source": [
    "Categorical variables were 'conditions', 'description' and 'icon'. I decided to plot 'icon' as that column represents how the weather was that day and it represents target variable as well. Description is just explaining weather conditions that day and it contains too much unique text to be plotted. \n",
    "\n",
    "As we can see, even though the weather was explained as moderate warm and humid, we can see that most common weather tag was rain.\n",
    "\n",
    "Again, I picked this plot to be interactive and added another interactive plot (to plot 'icon' against date, to get the type of weather for each day) for dashboard creating and added Grid Lines for better visibility and readbility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758f79a",
   "metadata": {},
   "source": [
    "#### Relationship between Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59943360",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = ['temp', 'humidity', 'windspeed', 'sealevelpressure', 'uvindex', 'severerisk', 'dew', 'visibility']\n",
    "corr_matrix = df[continuous_columns].corr()\n",
    "\n",
    "zmax = max(corr_matrix.values.max(), -corr_matrix.values.min())\n",
    "\n",
    "heatmap = go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=corr_matrix.columns,\n",
    "    y=corr_matrix.columns,\n",
    "    colorscale='RdBu',  \n",
    "    zmin=-zmax,\n",
    "    zmax=zmax,\n",
    "    colorbar=dict(title='Correlation'),\n",
    "    hoverongaps=False\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Correlation Heatmap of Continuous Variables',\n",
    "    xaxis=dict(title='Variables'),\n",
    "    yaxis=dict(title='Variables'),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[heatmap], layout=layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee5be2",
   "metadata": {},
   "source": [
    "As we can see, columns with higher positive correlation are temperature, dew, visibility and UV Index in combination with each other. \n",
    "\n",
    "\n",
    "I used coolwarm as color map and I was using cool colors (blue shades) for negative correlations and warm colors (red shades) for positive correlations is intuitive for most people, as it represents contrast in familiar way. Also, the stronger correlation is (or opposite), the color is darker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3713510",
   "metadata": {},
   "source": [
    "#### Temperature Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_columns = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike']\n",
    "\n",
    "box_traces = []\n",
    "for column in temperature_columns:\n",
    "    box_trace = go.Box(\n",
    "        y=df[column],\n",
    "        name=column,\n",
    "        boxmean='sd'  \n",
    "    )\n",
    "    box_traces.append(box_trace)\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Temperature Variation',\n",
    "    yaxis=dict(title='Temperature (°C)'),\n",
    "    xaxis=dict(title='Temperature Columns'),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=box_traces, layout=layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ecf07",
   "metadata": {},
   "source": [
    "This plot is showing us how temperatures vary. From what we can see, it looks that \"Feels like\" temperatures are quite aligned with actual temperatures, especially with max and min temperature ('feelslike' has higher span than 'temp'). \n",
    "\n",
    "There are no outliers. \n",
    "\n",
    "\n",
    "I picked this plot as it enables to compare temperature-related variables in a single plot. This helps in understanding the range and distribution of different temperature-related measurements. It also provides key statistics which helps assesing central tendency and skewness of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65800e60",
   "metadata": {},
   "source": [
    "#### Time-related Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'datetime' column to datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Create dropdown options for selecting variables\n",
    "dropdown_options = [{'label': col, 'value': col} for col in df.columns if col in [\n",
    "    'tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'snow', 'snowdepth',\n",
    "    'windgust', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility',\n",
    "    'solarradiation', 'solarenergy', 'uvindex', 'severerisk']\n",
    "]\n",
    "\n",
    "# Create a figure with 'temp' data as default\n",
    "fig = go.Figure(go.Scatter(x=df['datetime'], y=df['temp'], mode='lines'))\n",
    "\n",
    "# Update layout to include dropdown menu\n",
    "fig.update_layout(\n",
    "    title='Weather Trends over Time',    \n",
    "    updatemenus=[\n",
    "        {\n",
    "            'buttons': [\n",
    "                {\n",
    "                    'label': var,\n",
    "                    'method': 'update',\n",
    "                    'args': [{'y': [df[var]]}],\n",
    "                } for var in df.columns if var in [\n",
    "                    'tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'snow', 'snowdepth',\n",
    "                    'windgust', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility',\n",
    "                    'solarradiation', 'solarenergy', 'uvindex', 'severerisk']\n",
    "            ],\n",
    "            'direction': 'down',\n",
    "            'showactive': True,\n",
    "            'x': 0.01,\n",
    "            'xanchor': 'left',\n",
    "            'y': 1.1,\n",
    "            'yanchor': 'top'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the initial figure with 'temp' data\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a5ee6",
   "metadata": {},
   "source": [
    "I picked interactive plot with zoom funcionality due to large number of inputs, which wouldn't be readable in static plot. Users can zoom in and out for specific areas of interest, for closer examination of patterns or anomalies. \n",
    "Also, it provides ability to focus on smaller time intervals without losing the context of the entire dataset. I decided to add dropdown option, to give option to pick between most important weather variables.\n",
    "Added Grid and blue color for better readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cec2a3",
   "metadata": {},
   "source": [
    "#### Mean data per year compared to overall mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "df['year'] = df['datetime'].dt.year\n",
    "\n",
    "columns_of_interest = ['temp', 'humidity', 'snow', 'windspeed', 'uvindex']\n",
    "\n",
    "overall_mean = df[columns_of_interest].mean()\n",
    "\n",
    "mean_by_year = df.groupby('year')[columns_of_interest].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aded5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_of_interest:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.axhline(y=overall_mean[col], color='red', linestyle='-', label='Overall Mean')\n",
    "\n",
    "    bars = plt.bar(mean_by_year.index, mean_by_year[col], alpha=0.7, label=f'{col} Mean')\n",
    "    plt.title(f'{col} Mean Values per Year compared to Overall Mean')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3afc79",
   "metadata": {},
   "source": [
    "As we can see from these dashboards, temperature was the only one that was showing some sort of jump compared to overall mean. Mean temperature for 2018 was 2C higher than mean and almost 4C higher than mean of 2021 and 2022. \n",
    "\n",
    "Other than that, we can see there was no much snow in these years, only in 2022, which was one of the colder years.\n",
    "\n",
    "\n",
    "I used this plot to compare means values per year to the overall mean. It provides clear visual representation of how each zear's meand value for different variables compares to respective overall mean. It allows an easy comparison between the mean values and overall one. It also helps to identify if certain years were significantly different from others or from overall mean. \n",
    "\n",
    "I included Drid Lines and numeric values on top of the bars for better clarity and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "mean_temp = df.groupby(['year', 'month'])['temp'].mean().reset_index()\n",
    "\n",
    "pivot_mean_temp = mean_temp.pivot(index='month', columns='year', values='temp')\n",
    "pivot_mean_temp = pivot_mean_temp.rename(index=lambda x: calendar.month_abbr[x])\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=pivot_mean_temp.values.tolist(),\n",
    "    x=pivot_mean_temp.columns,\n",
    "    y=pivot_mean_temp.index,\n",
    "    colorscale='YlOrRd', \n",
    "    zmin=pivot_mean_temp.min().min(), \n",
    "    zmax=pivot_mean_temp.max().max(), \n",
    "    colorbar=dict(title='Mean Temperature'),\n",
    "    zhoverformat='.1f'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Mean Temperature per Month for Each Year',\n",
    "    xaxis=dict(title='Year'),\n",
    "    yaxis=dict(title='Month')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f79e1",
   "metadata": {},
   "source": [
    "I created this plot to get more in details temperature data for plot above this one. \n",
    "\n",
    "I used this type of plot as it effectively displays a matrix of data using variations in color to represent values. It's excellent for showcasing patterns in matrix-like datasets. By using color gradients, the heatmap makes it easy to interpret and compare mean temperature values. This color scheme helps to quickly grasp the variations in temperature across months and years, it's visible on the first sight which days were warmer than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e29bf",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf71a7",
   "metadata": {},
   "source": [
    "# Creating Interactive Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f276e4",
   "metadata": {},
   "source": [
    "### Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db52a65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ovaj je dobar\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.H1(\"Overview\", style={'font-size': '26px','text-align': 'center'}),\n",
    "            dcc.Markdown('''\n",
    "            The dataset, sourced from [Kaggle](https://www.kaggle.com/datasets/rukenmissonnier/weather-in-istanbul/data), \n",
    "            presents an extensive collection of weather parameters specific to Istanbul, \n",
    "            collected between April 7, 2018, and September 27, 2023. \n",
    "            With 2000 entries and 32 columns, it covers various weather attributes such as temperature, humidity, precipitation, \n",
    "            wind characteristics, and more. Link to local address for better visiblity: http://127.0.0.1:8050/\n",
    "            '''),\n",
    "\n",
    "            html.H3(\"Insights from Exploratory Data Analysis (EDA)\", style={'font-size': '20px', 'text-align': 'center'}),\n",
    "            dcc.Markdown('''\n",
    "            ###### Temperature Analysis\n",
    "            - **tempmax & tempmin**: Most commonly observed maximum temperature is approximately 30°C, while the typical minimum temperature hovers around 22°C. The average temperature observed falls close to 25°C.\n",
    "            - **temp**: The frequently recorded temperature tends to be around 25°C.\n",
    "            ###### Humidity and Wind Analysis\n",
    "            - **humidity**: The prevalent humidity levels are within the range of 70-75%.\n",
    "            - **windgust & windspeed**: Commonly recorded wind gusts range between 15 to 20 km/h.\n",
    "            ###### UV Index and Weather Conditions\n",
    "            - **uvindex**: The predominant UV index values are 8 and 9, indicating substantial exposure to UV radiation.\n",
    "            - **conditions & description**: The prevailing weather conditions suggest a moderately warm climate, with moderate humidity and occasional breezy conditions. The temperatures typically maintain a comfortable range without extreme hot or cold spells. Notably, the minimum temperature rarely drops below 0°C.\n",
    "            ###### Precautionary Measures\n",
    "            Given the fact we have a high count of the higher UV index values (8-10), it is advisable to take precautions against sun exposure during periods when the UV index is at its most common levels.\n",
    "            ''')\n",
    "        ], className='section'),\n",
    "    ], className='col-6'),\n",
    "\n",
    "        html.Div([\n",
    "            html.Div([\n",
    "                html.H1(\"Distribution of Weather Parameters\", style={'font-size': '24px', 'text-align': 'center'}),\n",
    "                dcc.Dropdown(\n",
    "                    id='weather-params-dropdown',\n",
    "                    options=[\n",
    "                        {'label': 'Temperature', 'value': 'temp'},\n",
    "                        {'label': 'Humidity', 'value': 'humidity'},\n",
    "                        {'label': 'Wind Gust', 'value': 'windgust'},\n",
    "                        {'label': 'UV Index', 'value': 'uvindex'},\n",
    "                        {'label': 'Max Temperature', 'value': 'tempmax'},\n",
    "                        {'label': 'Min Temperature', 'value': 'tempmin'}\n",
    "                ],\n",
    "                value='temp'  # Default value when the app starts\n",
    "            ),\n",
    "            dcc.Graph(id='weather-params-graph')\n",
    "        ], className='section')\n",
    "    ], className='col-6'),\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.H1(\"Mean Temperature per Month for Each Year\", style={'font-size': '22px', 'text-align': 'center'}),\n",
    "            dcc.Dropdown(\n",
    "                id='year-dropdown',\n",
    "                options=[{'label': str(year), 'value': year} for year in df['year'].unique()],\n",
    "                value=df['year'].min(),  # Default value when the app starts\n",
    "                clearable=False,  # Disable option to clear the selection\n",
    "                style={'width': '50%'}  # Adjust width of the dropdown\n",
    "            ),\n",
    "            dcc.Dropdown(\n",
    "                id='month-dropdown',\n",
    "                options=[{'label': calendar.month_abbr[month], 'value': month} for month in range(1, 13)],\n",
    "                value=8,  # Default value when the app starts (January)\n",
    "                clearable=False,  # Disable option to clear the selection\n",
    "                style={'width': '50%'}  # Adjust width of the dropdown\n",
    "            ),\n",
    "            dcc.Graph(id='mean-temp-graph')\n",
    "        ], className='section')\n",
    "    ], className='col-6'),\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.H1(\"Correlation Heatmap of Continuous Variables\", style={'font-size': '22px', 'text-align': 'center'}),\n",
    "            dcc.Dropdown(\n",
    "                id='corr-vars-dropdown-1',\n",
    "                options=[{'label': col, 'value': col} for col in df.columns],\n",
    "                value=['temp', 'humidity', 'snow', 'windspeed'],  # Default values when the app starts\n",
    "                multi=True  # Allow multiple selections\n",
    "            ),\n",
    "            dcc.Graph(id='corr-heatmap-2')\n",
    "        ], className='section'),\n",
    "    ], className='col-6'),\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.H1(\"Weather Trends over Time\", style={'font-size': '24px', 'text-align': 'center'}),\n",
    "            dcc.Dropdown(\n",
    "                id='weather-params-dropdown2',\n",
    "                options=[\n",
    "                    {'label': col, 'value': col} for col in [\n",
    "                        'tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'snow', 'snowdepth',\n",
    "                        'windgust', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility',\n",
    "                        'solarradiation', 'solarenergy', 'uvindex', 'severerisk']\n",
    "                ],\n",
    "                value='temp'  # Default value when the app starts\n",
    "            ),\n",
    "            dcc.Graph(id='weather-trends-graph', figure=fig)  # Your plot ID and initial figure\n",
    "        ], className='section'),\n",
    "    ], className='col-6'),\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.H1(\"Type of Weather for Each Day\", style={'font-size': '24px', 'text-align': 'center'}),\n",
    "            dcc.Graph(id='weather-type-graph')  # Your plot ID\n",
    "        ], className='section'),\n",
    "    ], className='col-6')\n",
    "], className='row')\n",
    "\n",
    "# Callbacks\n",
    "\n",
    "# Callback for weather parameters graph\n",
    "@app.callback(\n",
    "    Output('weather-params-graph', 'figure'),\n",
    "    [Input('weather-params-dropdown', 'value')]\n",
    ")\n",
    "def update_weather_params_graph(selected_param):\n",
    "    if selected_param is None:\n",
    "        default_param = 'temp'  # Set the default parameter to 'temp'\n",
    "        columns_for_histogram = [default_param]\n",
    "    else:\n",
    "        columns_for_histogram = [selected_param]\n",
    "\n",
    "    fig = px.histogram(df, x=columns_for_histogram, marginal=\"rug\", title=f\"Distribution of {columns_for_histogram[0].capitalize()}\")\n",
    "    fig.update_layout(\n",
    "        bargap=0.3,\n",
    "        width=800,\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        margin=dict(l=40, r=40, t=40, b=40),\n",
    "    )\n",
    "    return fig\n",
    "    \n",
    "\n",
    "# Callback for mean temperature\n",
    "@app.callback(\n",
    "    Output('mean-temp-graph', 'figure'),  # Assuming 'mean-temp-graph' is the ID of your heatmap plot\n",
    "    [Input('year-dropdown', 'value'),\n",
    "     Input('month-dropdown', 'value')]\n",
    ")\n",
    "def update_mean_temp_graph(selected_year, selected_month):\n",
    "    # Logic to update the heatmap plot based on selected year and month\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "\n",
    "    mean_temp = df.groupby(['year', 'month'])['temp'].mean().reset_index()\n",
    "    pivot_mean_temp = mean_temp.pivot(index='month', columns='year', values='temp')\n",
    "    pivot_mean_temp = pivot_mean_temp.rename(index=lambda x: calendar.month_abbr[x])\n",
    "\n",
    "    selected_temp = pivot_mean_temp[selected_year]\n",
    "    selected_month_str = calendar.month_abbr[selected_month]\n",
    "\n",
    "    fig = go.Figure(go.Heatmap(\n",
    "        z=pivot_mean_temp.values.tolist(),\n",
    "        x=pivot_mean_temp.columns,\n",
    "        y=pivot_mean_temp.index,\n",
    "        colorscale='YlOrRd',\n",
    "        zmin=pivot_mean_temp.min().min(),\n",
    "        zmax=pivot_mean_temp.max().max(),\n",
    "        colorbar=dict(title='Mean Temperature'),\n",
    "        zhoverformat='.1f'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f'Mean Temperature for {selected_month_str} {selected_year}',\n",
    "        template='plotly_white',\n",
    "        xaxis=dict(title='Year'),\n",
    "        yaxis=dict(title='Month'),\n",
    "        annotations=[\n",
    "            dict(\n",
    "                x=selected_year,\n",
    "                y=pivot_mean_temp.index.get_loc(selected_month_str),  # Getting index location of selected month\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                text=f\"Mean Temp: {selected_temp.loc[selected_month_str]:.1f}\",\n",
    "                showarrow=True,\n",
    "                arrowhead=7,\n",
    "                ax=0,\n",
    "                ay=-40\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# Callback for corr-heatmap-2\n",
    "@app.callback(\n",
    "    Output('corr-heatmap-2', 'figure'),  # Assuming 'corr-heatmap-2' is the ID of your heatmap\n",
    "    [Input('corr-vars-dropdown-1', 'value')]\n",
    ")\n",
    "def update_corr_heatmap(selected_variables):\n",
    "    corr_matrix = df[selected_variables].corr()\n",
    "\n",
    "    zmax = max(corr_matrix.values.max(), -corr_matrix.values.min())\n",
    "\n",
    "    heatmap = go.Heatmap(\n",
    "        z=corr_matrix.values,\n",
    "        x=corr_matrix.columns,\n",
    "        y=corr_matrix.columns,\n",
    "        colorscale='RdBu',\n",
    "        zmin=-zmax,\n",
    "        zmax=zmax,\n",
    "        colorbar=dict(title='Correlation'),\n",
    "        hoverongaps=False\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        template='plotly_white',\n",
    "        xaxis=dict(title='Variables'),\n",
    "        yaxis=dict(title='Variables'),\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[heatmap], layout=layout)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Callback for weather-trends-graph\n",
    "# Callback for updating the weather trends graph\n",
    "@app.callback(\n",
    "    Output('weather-trends-graph', 'figure'),\n",
    "    [Input('weather-params-dropdown2', 'value')]\n",
    ")\n",
    "def update_weather_trends_graph(selected_param):\n",
    "    # Logic to update the figure based on the selected parameter\n",
    "\n",
    "    # Assuming df is your DataFrame and 'datetime' is the datetime column\n",
    "    updated_fig = go.Figure(go.Scatter(x=df['datetime'], y=df[selected_param], mode='lines'))\n",
    "\n",
    "    # Further customization of the updated_fig based on selected_param...\n",
    "    updated_fig.update_layout(\n",
    "        title=f\"Weather Trends for {selected_param}\",\n",
    "        template='plotly_white',\n",
    "        xaxis=dict(title='Date'),\n",
    "        yaxis=dict(title=selected_param)\n",
    "    )\n",
    "\n",
    "    return updated_fig\n",
    "\n",
    "# Callback for updating the \"Type of Weather for Each Day\" plot\n",
    "@app.callback(\n",
    "    Output('weather-type-graph', 'figure'),\n",
    "    [Input('weather-params-dropdown2', 'value')]\n",
    ")\n",
    "def update_weather_type_graph(selected_param):\n",
    "    # Assuming df is your DataFrame and 'datetime' is the datetime column\n",
    "    # Replace 'icon' with your actual column name for weather type\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    fig = px.scatter(df, x='datetime', y='icon', labels={'icon': 'Icon', 'datetime': 'Date'})\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis={'title': 'Date'},\n",
    "        yaxis={'title': 'Icon'},\n",
    "        height=400,\n",
    "        width=700,\n",
    "        template='plotly_white',\n",
    "        margin=dict(l=40, r=40, t=40, b=40),\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, dev_tools_ui=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f37a6",
   "metadata": {},
   "source": [
    "As per assignment, we were supposed to create a interactive dashboard which will include 3 rows and 2 columns, making total of 6 sections. When it comes to visualisation compontents, in my workbook, I used Heatmap, Line Chart, Histograms, Bar Chart, Stacked Bar Chart, Boxplot and Scatter plots to visualise my dataset. Also, I used Interactive features like dropdowns and interactive plots.\n",
    "\n",
    "My dashboard followed those instructions, it consist of 6 sections (3 rows and 2 columns). First section is overview of the dataset (text section), while other 5 are interactive plots (4 of them with dropdowns, one without.)\n",
    "\n",
    "1. \"Overview\" is a text section, it's giving all the relevant on the dataset I picked, as well as, link to it's source. Also, sums up all the conclusions I made by performing the EDA. \n",
    "\n",
    "\n",
    "2. \"Distribution of Weather Parameters\" is histogram, created to give insights on distribution of the weather parameters. I used this type of the plot as it's ideal to display the frequency of weather variables like temperature, humidity etc and it helps to recognise patterns and which values are most frequent. It also helps to understand ranges of weather conditions. I also included dropdown with variables, so users can pick variables that interest them.\n",
    "\n",
    "\n",
    "3. \"Mean Temperature per Month for Each Year\" is a heatmap type of plot and it displays mean temperature variations across on month level, for 5 years. I picked this type of plot as it's perfect for illustrating how mean temperature varies over time, helping to identify seasonal patterns. It also shows variations effectively, allowing viewer to compare temperatures across different time period. I used colorscale='YlOrRd' as it makes visible to bare eye which periods were warmer and which ones were cooloer by the darker and lighter colors. This plot is also interactive, with dropdown allowing audience to pick a year and month of the interest.\n",
    "\n",
    "\n",
    "4. \"Correlation Heatmap of Continuous Variables\" is a Heatmap (Correlation Matrix) type of plot. I picked this type of plot as it shows relationship between continuous variables in simple and understandable way. It's ideal for understanding the strengt of correlations between weather parameters like temperature, humidity, wind speed etc. Also, it helps to identify which variables are strongly correlated and by that, impacting each other. By having this plot, audience will be able, for example, to see strong correlation between temperature and UVIndex. I picked this colorscale as it clearly visible that variables with darker colors are in stronger correlation than ones in lighter colors. Stronger correlation is, the color is darker and vice-versa. Also, I made this plot interactive and added dropdown as well, with list of most important variables so audience can pick variables of their interest. \n",
    "\n",
    "\n",
    "5. \"Weather Trends over Time\" is time-series plot, Line Chart type. Purpose of this plot is to show trends or patterns over a continuous time period. It's really useful for visualising how weather parameters (eg. temperature, humidity etc) change or trend over time. It also helps audience to identify patterns, seasonality or long-term trends in the weather data. This plot is interactive as well, so audience can zoom in specific period of interest, but also they can pick from dropdown parameter of their interest, to see it's continous sequence and to percieve trends and changes more intuitively. \n",
    "\n",
    "\n",
    "6. \"Type of Weather for Each Day\" is categorical time series plot. I picked this plot to give audience more insights on what kind of weather was recorded each day, in case they want to compare other weather parameters with actual weather for that specific day. I used this type of plot as it's great for presenting categorical data like weather conditions (e.g. sunny, rainy, cloudy etc.) observed daily. It also helps to understand the distribution and frequency of each weather condition over the time. This plot was also interactive, due to high number of inputs, to make it readable. \n",
    "\n",
    "\n",
    "These plots together offer a comprehensive view of the weather dataset, covering distribution, trends over time, relationships between variables and correlations among continuous variables, ensuring a holistic understanding for audience. The selection of plots still effectively covers various asprects of the weather dataset, providing audience with multiple perspectives to explore and understand the data's nuances.\n",
    "\n",
    "\n",
    "Choice of colors: I used blue color for bars, dots and lines as it's percieved as calm color, it's easy on eyes and can create sense of comfort for viewers, making it pleasant to look at for extended periods. In cases I had more options to pick at same time, I had to bring in other colors as well, for better visibility, but blue was main color in dashboard plots. Also, I was adding Grid Lines, as per instructions from previous assignment again, for better visibility, as well as using white background. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bdaae",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a6a1e",
   "metadata": {},
   "source": [
    "# Machine Learning for Business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff135c6",
   "metadata": {},
   "source": [
    "## Time series analysis\n",
    "\n",
    "\n",
    "Times series analysis involves analyzing data points collected or recorded at specific time intervals. It is used for forecasting, understanding underlying patterns and making predictions based on the time-dependent data. In machine learning, various models like ARMA, ARIMA, SARIMA etc. are applied to time series for prediction. \n",
    "\n",
    "\n",
    "Time series used on my specific dataset: \n",
    "\n",
    "Temporal Patterns and Trends:\n",
    "Time series helps in uncovering patterns, trends, and seasonality within the data. It allows us to understand how weather attributes like temperature, humidity, wind characteristics, etc. change over time. \n",
    "\n",
    "Forecasting and Predictions:\n",
    "Time series models are used to forecast future values based on historical patterns. For instance, predicting future temperature trends or the likelihood of certain weather conditions occurring based on past data.\n",
    "\n",
    "Detecting Anomalies or Outliers:\n",
    "Analyzing time series data helps in identifying unusual or anomalous events. Sudden spikes in temperature, unexpected weather patterns, or irregularities can be detected through time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac04226",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10079971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "df[\"windspeed\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e516de6",
   "metadata": {},
   "source": [
    "### ADF Test\n",
    "\n",
    "##### Purpose of Augmented Dickey-Fuller Test:\n",
    "The ADF test is used to determine wheather a given time series is stationary or not. \n",
    "Stationarity is a crucial assumption in time series analysis, and the ADF test helps in confirming or rejecting the presence of a unit root in the data. A unit root suggests non-stationarity. \n",
    "\n",
    "Stationarity refers to a time series where statistical properties, like mean, variance, and autocorrelation, remain constant over time. In simpler terms, it doesn't exhibit long-term trends, and its statistical properties don't change with time.\n",
    "\n",
    "Stationarity is vital in time series analysis because many forecasting models assume it. It simplifies analysis, making predictions more reliable. Non-stationary data might give misleading insights or forecasts, making modeling more challenging.\n",
    "\n",
    "\n",
    "I will perform ADF test on 'windspeed' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25425763",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(df['windspeed'], autolag='AIC')\n",
    "\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'   {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df6f07",
   "metadata": {},
   "source": [
    "#### Results Analysis: \n",
    "\n",
    "ADF Statistic: The ADF Statistic value is -16.03. This value is significantly lower (more negative) than the critical values at 1%, 5%, and 10% levels, indicating strong evidence against the null hypothesis. This suggests that the 'windspeed' time series is stationary.\n",
    "\n",
    "p-value: The p-value of approximately 6.06e-29 is well below the typical significance levels (e.g., 0.05), providing further evidence against the null hypothesis of non-stationarity. The low p-value indicates a high level of confidence in rejecting the null hypothesis in favor of stationarity.\n",
    "\n",
    "Critical Values: All critical values at 1%, 5%, and 10% levels are more negative than the ADF Statistic, confirming the statistical significance of the test results and supporting the stationarity of the 'windspeed' time series.\n",
    "\n",
    "With the ADF test indicating that the 'windspeed' time series is stationary, it suggests that there might not be a need for differencing or other transformations to make the data stationary before proceeding with time series modeling or analysis. This stationary characteristic simplifies the modeling process and makes it more suitable for various time series modeling techniques without requiring additional adjustments for stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68223f",
   "metadata": {},
   "source": [
    "### EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['datetime'], df['windspeed'], color='blue')\n",
    "plt.title('Windspeed Time Series for 2022')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Windspeed')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedd94e",
   "metadata": {},
   "source": [
    "### Identification of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(211)\n",
    "plot_acf(df['windspeed'], lags=40, ax=plt.gca(), color='blue')\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "\n",
    "plt.subplot(212)\n",
    "plot_pacf(df['windspeed'], lags=40, ax=plt.gca(), color='blue')\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3444ef31",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "\n",
    "I decided to go with ARIMA based on observed characteristics of the windspeed time series data, particulary from ACF and PACF plot, along with some previous assumptions:\n",
    "\n",
    "1. ACF and PACF Patterns: The significant spikes observed in the PACF plot at the first two lags and the ACF plot indicating a slow decay in autocorrelation suggest a potential autoregressive (AR) behavior at these lags. This pattern aligns with the characteristics that ARIMA models aim to capture.\n",
    "\n",
    "2. Stationarity: As per the earlier analysis using the Augmented Dickey-Fuller (ADF) test, the 'windspeed' series was found to be stationary. ARIMA models are applicable to stationary time series data, and stationary series tend to exhibit consistent behavior over time, making them suitable for ARIMA modeling.\n",
    "\n",
    "3. Time Dependency: ARIMA models are designed to capture time dependencies in data, especially those with observed autocorrelation patterns (as seen in ACF and PACF plots).\n",
    "\n",
    "4. Forecasting: Since assignment requires one-step-ahead forecasting, ARIMA is used in short forcastings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe6eec",
   "metadata": {},
   "source": [
    "### Finding the Best Parameters\n",
    "\n",
    "\n",
    "As my dataset is already stationary, I might not need to include differencing in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_vals = []\n",
    "for p in range(5):\n",
    "    for q in range(5):\n",
    "        model = sm.ARIMA(df['windspeed'], order=(p, 0, q))\n",
    "        try:\n",
    "            fitted_model = model.fit()\n",
    "            aic = fitted_model.aic\n",
    "            aic_vals.append([aic, p, q])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "aic_vals.sort()\n",
    "\n",
    "print(aic_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c984eb7",
   "metadata": {},
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller(train_data[\"windspeed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df831a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= [12, 5])\n",
    "plt.plot(train_data[\"windspeed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(train_data[\"windspeed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(train_data[\"windspeed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c479e3b",
   "metadata": {},
   "source": [
    "### Fitting ARIMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ff394",
   "metadata": {},
   "source": [
    "Parameters: p, d, q = 2, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ebeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, d, q = 4, 0, 4  \n",
    "arima_model = sm.ARIMA(train_data['windspeed'], order=(p, d, q))\n",
    "fitted_model = arima_model.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352fad77",
   "metadata": {},
   "source": [
    "### Further Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b720334",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.plot_diagnostics(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c26e1",
   "metadata": {},
   "source": [
    "Parameters: p, d, q = 1, 0, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, d, q = 1, 0, 4  \n",
    "arima_model = sm.ARIMA(train_data['windspeed'], order=(p, d, q))\n",
    "fitted_model2 = arima_model.fit()\n",
    "fitted_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2984fef",
   "metadata": {},
   "source": [
    "### Further Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model2.plot_diagnostics(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36fe901",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed2d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = df['windspeed'].tail(10)\n",
    "\n",
    "forecasted_values_df = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    fitted_model = sm.ARIMA(pd.concat([df['windspeed'].head(len(df) - 10 + i), actual_values.head(i + 1)]), order=(4, 0, 4)).fit()\n",
    "    \n",
    "    next10_values = fitted_model.forecast(steps=10)\n",
    "    \n",
    "    next10_values.index = range(actual_values.index[-1] + 1, actual_values.index[-1] + 11)\n",
    "    \n",
    "    forecasted_values_df = pd.concat([forecasted_values_df, next10_values])\n",
    "\n",
    "forecasted_values_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_value = df['windspeed'].tail(1)\n",
    "\n",
    "fitted_model = sm.ARIMA(df['windspeed'], order=(4, 0, 4)).fit() \n",
    "\n",
    "forecast_value = fitted_model.forecast(steps=1)\n",
    "forecast_value.index = [actual_values.index[-1] + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6921f0e",
   "metadata": {},
   "source": [
    "### Forcasted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af75b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last 10 actual values:\")\n",
    "print(actual_values.to_string)\n",
    "print(\"-\" * 30)  \n",
    "print(\"Next 10 forecasted windspeed values:\")\n",
    "print(next10_values.to_string)\n",
    "print(\"-\" * 30)  \n",
    "\n",
    "print(\"Last actual value:\")\n",
    "print(last_value.to_string)\n",
    "print(\"-\" * 30)  \n",
    "\n",
    "print(\"Next forecasted value:\")\n",
    "print(forecast_value.to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f13e4",
   "metadata": {},
   "source": [
    "The difference in the single forecasted value compared to the subsequent ten forecasted values may arise due to the model's estimation. When requesting only the next value, the model is making a prediction solely for that immediate step, whereas when requesting the next ten values, the model might consider longer-term trends and patterns, resulting in potentially different forecasts.\n",
    "Adjusting the model parameters or training it with different configurations might influence these variations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755c0eb",
   "metadata": {},
   "source": [
    "### Forecast Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05531036",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_errors = []\n",
    "\n",
    "for i in range(10):\n",
    "    forecast = fitted_model.forecast(steps=1)\n",
    "    \n",
    "    forecast_errors.append(actual_values.values[i] - forecast)\n",
    "    \n",
    "    fitted_model = sm.ARIMA(pd.concat([df['windspeed'].head(len(df) - 10 + i), actual_values.head(i)]), order=(4, 0, 4)).fit()\n",
    "\n",
    "print(\"Forecast Errors:\")\n",
    "print(forecast_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e98c6a",
   "metadata": {},
   "source": [
    "### Model Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(actual_values, next10_values)\n",
    "\n",
    "rmse = mean_squared_error(actual_values, next10_values, squared=False)\n",
    "\n",
    "r_squared = r2_score(actual_values, next10_values)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfa2f6",
   "metadata": {},
   "source": [
    "### Forecast on different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_new = 1 \n",
    "d_new = 0 \n",
    "q_new = 4\n",
    "\n",
    "forecasted_values_df_2 = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    fitted_model2 = sm.ARIMA(pd.concat([df['windspeed'].head(len(df) - 10 + i), actual_values.head(i + 1)]), order=(p_new, d_new, q_new)).fit()\n",
    "    \n",
    "    next10_values_2 = fitted_model2.forecast(steps=10)\n",
    "    \n",
    "    next10_values_2.index = range(actual_values.index[-1] + 1, actual_values.index[-1] + 11)\n",
    "    \n",
    "    forecasted_values_df_2 = pd.concat([forecasted_values_df_2, next10_values_2])\n",
    "\n",
    "forecasted_values_df_2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_value = df['windspeed'].tail(1)\n",
    "\n",
    "fitted_model2 = sm.ARIMA(df['windspeed'], order=(p_new, d_new, q_new)).fit() \n",
    "\n",
    "forecast_value_new = fitted_model2.forecast(steps=1)\n",
    "forecast_value_new.index = [actual_values.index[-1] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last 10 actual values:\")\n",
    "print(actual_values.to_string())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Next 10 forecasted windspeed values:\")\n",
    "print(next10_values_2.to_string())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Last actual value:\")\n",
    "print(last_value.to_string())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Next forecasted value:\")\n",
    "print(forecast_value_new.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_errors = []\n",
    "\n",
    "for i in range(10):\n",
    "    forecast = fitted_model2.forecast(steps=1)\n",
    "    \n",
    "    forecast_errors.append(actual_values.values[i] - forecast)\n",
    "    \n",
    "    fitted_model2 = sm.ARIMA(pd.concat([df['windspeed'].head(len(df) - 10 + i), actual_values.head(i)]), order=(p_new, d_new, q_new)).fit() \n",
    "\n",
    "print(\"Forecast Errors:\")\n",
    "print(forecast_errors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(actual_values, next10_values2)\n",
    "rmse = mean_squared_error(actual_values, next10_values2, squared=False)\n",
    "r_squared = r2_score(actual_values, next10_values2)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2473a8",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f28d8",
   "metadata": {},
   "source": [
    "#### Last 10 values vs one-step-ahead value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "forecast_index = actual_values.index[-1] + 1\n",
    "\n",
    "plt.plot(actual_values.index, actual_values, label='Actual', color='blue') \n",
    "\n",
    "plt.plot([actual_values.index[-1], forecast_index], [actual_values.iloc[-1], forecast_value.iloc[0]], label='Forecast', color='red')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Wind speed')\n",
    "plt.title('Last 10 values vs Forecasted Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8e406",
   "metadata": {},
   "source": [
    "#### Last 10 values vs forecasted 10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73223ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(actual_values.index, actual_values, label='Actual', color='blue')\n",
    "\n",
    "forecast_index = actual_values.index[-1] + 1\n",
    "\n",
    "forecast_values_index = [actual_values.index[-1]] + list(range(forecast_index, forecast_index + 10))\n",
    "forecast_values = [actual_values.iloc[-1]] + list(next10_values)\n",
    "\n",
    "plt.plot(forecast_values_index, forecast_values, label='Next 10 Forecast', color='red', marker='o')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual vs Next 10 Forecasted Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf3f4c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1db22a",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The Mean Absolute Error (2.62) and Root Mean Squared Error (3.22) values suggest the model's predictive accuracy in terms of absolute and squared errors, but it still show that the model's performance might have room for improvement.\n",
    "\n",
    "Lower values for MAE and RMSE are better, indicating smaller prediction errors. For R², a value closer to 1 is better, as it suggests the model explains a larger portion of the variability in windspeed. However, in my case, the negative R² indicates that my model might not be a great fit for this specific data, possibly due to the inadequacy of the model or the complexity of the underlying patterns in the data.\n",
    "\n",
    "\n",
    "Nature of the variable I used for the predicting can significantly impact the model's performance. Wind speed data often exhibits high variability and can be influenced by various complex factors like weather patterns, geography, and seasonal variations. The inherent variability in wind speed data can make it challenging to capture precise patterns, impacting the model's predictive accuracy. considering the complexities in wind data, advanced modeling techniques should be explored, potentialy incorporating additional relevant features. \n",
    "\n",
    "I was testing this model with different parameters, scores were pretty much the same, even worse than these.\n",
    "\n",
    "I did different approaches, changed size of dataset few times (tried with dataset based on weekly average for all the years, then with dataset for one year only due to computational requirements just to finish with complete dataset, for all the years to make model better on training.)\n",
    "\n",
    "Also, I was testing different models and different features: I was working on temperature feature where I was using SARIMA, but had to give up due to computational requirements and seasonality of temperature dataset (I tried reducing datasets but it was still taking too long). \n",
    "\n",
    "At the end, I downloaded new dataset completely and was considering to try in that way, by bringing stock values dataset and performing ARMA model, but decided to stay with this one and not to change dataset for every task.\n",
    "\n",
    "\n",
    "Even though my model provides some predictive capability, the scores suggest considering alternative modeling techniques to better capture complexities in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a4f3c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c446ca7",
   "metadata": {},
   "source": [
    "# Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeebf75",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('climate_change_tweets.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df. info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c96ad",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c0ea4",
   "metadata": {},
   "source": [
    "#### Converting text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lowercase_Text'] = df['Embedded_text'].apply(lambda x: x.lower())\n",
    "\n",
    "print(\"Lowercase Text:\")\n",
    "print(df['Lowercase_Text'].head())\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e65c6",
   "metadata": {},
   "source": [
    "#### Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|@[^\\s]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['No_Special_Chars_Text'] = df['Lowercase_Text'].apply(remove_special_chars)\n",
    "\n",
    "print(\"Text without Special Characters, URLs, and Usernames:\")\n",
    "print(df['No_Special_Chars_Text'].head())\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d788057",
   "metadata": {},
   "source": [
    "#### Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenized_Text'] = df['No_Special_Chars_Text'].apply(word_tokenize)\n",
    "\n",
    "print(\"Tokenized Text:\")\n",
    "print(df['Tokenized_Text'].head(10))\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b794a0",
   "metadata": {},
   "source": [
    "#### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ab82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['Without_Stopwords_Text'] = df['Tokenized_Text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "print(\"Text without Stopwords:\")\n",
    "print(df['Without_Stopwords_Text'].head(10))\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b793d7",
   "metadata": {},
   "source": [
    "### Text Categorisation / Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5235a6",
   "metadata": {},
   "source": [
    "#### Data Labeling\n",
    "\n",
    "In order not to do manual labeling or importing external labeled datasets, I will use Unsupervise Sentiment Analyisis. More precisely, I will use TextBlob to calculate sentiment polarity scores for each tweet text. The 'Sentiment_TextBlob' column will contain the polarity scores ranging from -1 (negative) to 1 (positive), indicating the sentiment polarity of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment_TextBlob'] = df['Embedded_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Function to map sentiment scores to categorical labels\n",
    "def map_sentiment(score):\n",
    "    if score > 0.1:\n",
    "        return 'positive'\n",
    "    elif score < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "print(df[['Embedded_text', 'Sentiment_TextBlob']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2f055",
   "metadata": {},
   "source": [
    "I will create discrete categorical labels directly from the sentiment polarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e36b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Sentiment_TextBlob'].apply(map_sentiment)\n",
    "print(df[['Embedded_text', 'Sentiment']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65517c",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "After I have obtained the labeled dataset with sentiment scores assigned to the tweets, the next step would be feature extraction from the preprocessed text data. This step involves transforming the textual data into numerical features that machine learning models can understand and utilise for training. I will use TF-IDF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Text'] = df['Without_Stopwords_Text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48828fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ac969",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = tfidf_vectorizer.fit_transform(df['Processed_Text'])\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d9ae9",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "\n",
    "For this step, I'll use the Multinomial Naive Bayes classifier, a common choice for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_features\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f608a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9fc21",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824350f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e1dd4",
   "metadata": {},
   "source": [
    "The model's accuracy of approximately 59.61% suggests that it correctly predicts the sentiment for around 59.61% of the tweets in the test set. However, the classification report reveals that the model's performance varies across different sentiment classes.\n",
    "\n",
    "It appears that the model has challenges distinguishing 'negative' sentiments, as it has very low recall and F1-score for this class, indicating that it correctly identifies very few 'negative' sentiments compared to the actual instances present in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc00cec",
   "metadata": {},
   "source": [
    "#### Improving model scores\n",
    "\n",
    "Since dataset suffers from significant class imbalance (fewer 'negative' sentiments), addressing this issue could be beneficial and I will do it by using Oversampling technique, to help balance the representation of different sentiment classes, potentially improving the model's ability to learn from underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_train_oversampled, y_train_oversampled = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216359f4",
   "metadata": {},
   "source": [
    "#### Retraining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bacb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a9601",
   "metadata": {},
   "source": [
    "The drop in accuracy might indicate that the model, after oversampling, struggles to generalize well to unseen data or that the approach to balancing the classes did not sufficiently improve the model's overall performance.\n",
    "\n",
    "I will try with Fine-tuning Hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc60f3",
   "metadata": {},
   "source": [
    "#### Fine-tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.1, 0.5, 1.0, 1.5, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': alpha_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(nb_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c14c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d5fea",
   "metadata": {},
   "source": [
    "#### Exploring Advanced Models\n",
    "\n",
    "\n",
    "Advanced models might capture more complex relationships within the text data, allowing for better representation of sentiments. I will try to improve scores with SVM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee25d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='linear', C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm = svm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7644071",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
    "report_svm = classification_report(y_test, predictions_svm)\n",
    "\n",
    "print(f\"Accuracy (SVM): {accuracy_svm}\")\n",
    "print(\"Classification Report (SVM):\")\n",
    "print(report_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc165df",
   "metadata": {},
   "source": [
    "The SVM model's overall performance, as indicated by accuracy and F1-scores, has shown significant enhancement compared to the previous model attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d403a89",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfad1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "#grid_search.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeee463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params = grid_search.best_params_\n",
    "#best_model_svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147031af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_svm.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_svm_tuned = best_model_svm.predict(X_test)\n",
    "#accuracy_svm_tuned = accuracy_score(y_test, predictions_svm_tuned)\n",
    "#report_svm_tuned = classification_report(y_test, predictions_svm_tuned)\n",
    "\n",
    "#print(f\"Best Hyperparameters: {best_params}\")\n",
    "#print(f\"Accuracy (Tuned SVM): {accuracy_svm_tuned}\")\n",
    "#print(\"Classification Report (Tuned SVM):\")\n",
    "#print(report_svm_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa81cb",
   "metadata": {},
   "source": [
    "The overall performance of the tuned SVM model, as indicated by accuracy and F1-scores, shows some improvement compared to the default SVM model without hyperparameter tuning but it seems that's the best accuracy can go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac842c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a0987",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "Topic modeling is a technique used to discover abstract topics present in a collection of documents. One popular algorithm for topic modeling is Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a4cb9",
   "metadata": {},
   "source": [
    "#### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9782675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "              if token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "    return ' '.join(tokens) if tokens else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Text'] = df['Embedded_text'].fillna('').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "doc_term_matrix = vectorizer.fit_transform(df['Processed_Text'])\n",
    "print(f\"Shape of Document-Term Matrix: {doc_term_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c582bf6",
   "metadata": {},
   "source": [
    "#### Applying Latent Dirichlet Allocation (LDA) for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3400104",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31305637",
   "metadata": {},
   "source": [
    "#### Extracting Topics and Associated Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10 \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"Top {num_top_words} words for each topic:\")\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ca14f",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7f58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordclouds(model, feature_names):\n",
    "    num_topics = len(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        word_freq = {feature_names[i]: topic[i] for i in topic.argsort()[:-15 - 1:-1]} \n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Word Cloud - Topic {topic_idx + 1}\")\n",
    "        plt.show()\n",
    "\n",
    "generate_wordclouds(lda, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63304e6",
   "metadata": {},
   "source": [
    "#### Topic_based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dominant_topic(model, doc_term_matrix):\n",
    "    dominant_topics = model.transform(doc_term_matrix)\n",
    "    return dominant_topics.argmax(axis=1) + 1  # Adding 1 to start topics from index 1\n",
    "\n",
    "df['Dominant_Topic'] = assign_dominant_topic(lda, doc_term_matrix)\n",
    "\n",
    "topic_counts = df['Dominant_Topic'].value_counts().sort_index()\n",
    "print(\"Tweet Count in Each Dominant Topic:\")\n",
    "print(topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(topic_counts, x=topic_counts.index, y=topic_counts.values, labels={'x': 'Dominant Topic', 'y': 'Tweet Count'},\n",
    "             title='Tweet Count in Each Dominant Topic')\n",
    "fig.update_traces(marker_color='skyblue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e56970",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(1, 6):\n",
    "    topic_tweets = df[df['Dominant_Topic'] == topic]['Embedded_text']\n",
    "    df[f'Topic_{topic}_Sentiment'] = topic_tweets.apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(1, 6):\n",
    "    print(f\"Sentiment Distribution in Topic {topic}:\")\n",
    "    print(df[f'Topic_{topic}_Sentiment'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a2eef",
   "metadata": {},
   "source": [
    "Topics 4 and 5 predominantly exhibited a higher volume of positive sentiments compared to negative sentiments, with Topic 4 displaying the highest positive sentiment count. Conversely, Topic 4 also had a notable count of negative sentiments, showcasing a more balanced sentiment distribution. Topics 1, 2, and 3 showcased varying degrees of positive, negative, and neutral sentiments, with Topic 3 displaying a higher count of positive sentiments. Overall, the sentiment distributions indicate a mixed sentiment landscape surrounding climate change discussions, encompassing positive, negative, and neutral expressions across the identified topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82890521",
   "metadata": {},
   "source": [
    "#### Temporal Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc99ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data = df['Text'].resample('D').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(resampled_data, x='Timestamp', y='Text', title='Tweet Volume Related to Climate Change Over Time')\n",
    "fig.update_xaxes(title='Time')\n",
    "fig.update_yaxes(title='Tweet Count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea475100",
   "metadata": {},
   "source": [
    "As we can see from the plot, there was significant spike in tweets on july 12th, 2022 with 175 tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e3d8b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43419d10",
   "metadata": {},
   "source": [
    "### Document Summarisation\n",
    "\n",
    "\n",
    "Document summarization involves condensing a document's main points or content into a shorter representation while preserving its essence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca37bb",
   "metadata": {},
   "source": [
    "#### Extractive Summarization with TextRank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize(text, ratio=0.2)\n",
    "\n",
    "print(\"Extractive Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82860486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
